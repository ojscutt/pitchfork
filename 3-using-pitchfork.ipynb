{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca44dac-c4e3-4829-a7a8-91ae0b697f34",
   "metadata": {},
   "source": [
    "# 3) Using Pitchfork\n",
    "Here's where I'm going to chuck the other cool stuff you can do with `pitchfork` that isn't necessary in the main sampler!\n",
    "\n",
    "This is why I chose to structure the functions in this repo around importing pitchfork within the notebook first, and then passing it to `pitchfork_sampler` - this way, it shows how you can import and use the emulator directly.\n",
    "\n",
    "I'll give a couple of rough examples of using pitchfork here:\n",
    "1. Making predictions with `pitchfork.predict`\n",
    "2. Emulating densely sampled stellar evolutionary tracks\n",
    "3. Posterior predictive check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d71cf-9635-4021-998a-9216ac05d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from scripts import pitchfork_compile, pitchfork_sampler, posterior_plot\n",
    "\n",
    "### compile pitchfork\n",
    "\n",
    "with open('pitchfork/pitchfork.json', 'r') as fp:\n",
    "    pitchfork_dict = json.load(fp)\n",
    "\n",
    "with open('pitchfork/pitchfork_info.json', 'r') as fp:\n",
    "    pitchfork_info = json.load(fp)\n",
    "\n",
    "pitchfork_cov = np.loadtxt('pitchfork/pitchfork_covariance.txt')\n",
    "\n",
    "pitchfork = pitchfork_compile(pitchfork_dict, pitchfork_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb485afc-6f4f-48f2-bb22-42e0146ad506",
   "metadata": {},
   "source": [
    "## 1. Predicting with `pitchfork`\n",
    "Making predictions with `pitchfork` is super easy once imported like I've done above!\n",
    "\n",
    "Pitchfork expects inputs with dimensions (n, 5), where n is the batch size (which can be extremely large!). This is important - `pitchfork` will complaing if you try to predict on a single point with dimension (5) - you need to make sure it has dimensions (1,5).\n",
    "\n",
    "The 5 inputs are `['initial_mass', 'initial_Zinit', 'initial_Yinit', 'initial_MLT', 'star_age']` *IN THAT ORDER*! This is also very important - `pitchfork` will **NOT** complain if you mix up your inputs and will confidently give a prediction with no warning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35590a37-8466-430e-bc04-5c7f9d9c1753",
   "metadata": {},
   "source": [
    "### 1.1. Single Prediction\n",
    "Let's see how we'd make a prediction for a single point with pitchfork:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc10079-8d4c-4014-914e-4e3aac7f031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_point_dict = {'initial_mass': 1,\n",
    "'initial_Zinit': 0.01,\n",
    "'initial_Yinit': 0.26,\n",
    "'initial_MLT': 2.12,\n",
    "'star_age': 4.52,\n",
    "}\n",
    "\n",
    "single_point_array = np.array(list(single_point_dict.values()))\n",
    "\n",
    "print(f\"single_point_array has dimensions: {single_point_array.shape} - pitchfork won't like that...\")\n",
    "\n",
    "single_point_array_fixed = np.array([list(single_point_dict.values())])\n",
    "\n",
    "print(f\"single_point_array_fixed has dimensions: {single_point_array_fixed.shape} - much better!\")\n",
    "\n",
    "print('')\n",
    "print('Passing to pitchfork to predict:')\n",
    "pitchfork.predict(single_point_array_fixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd35bb-dc27-4adc-a070-052a5b37c394",
   "metadata": {},
   "source": [
    "This looks a little convoluted, but actually this can easily be done on one line, like:\n",
    "```\n",
    "pitchfork.predict([[1,2,3,4,5]])\n",
    "```\n",
    "works absolutely fine - the important thing is to remember the extra `[]` to add the batch dimension that `pitchfork` expects.\n",
    "\n",
    "The outputs from `pitchfork` have dimensions (n,39), with the second dimension being the 39 different outputs that `pitchfork` was trained to predict. These are:\n",
    "```\n",
    "[\"calc_effective_T\", \"luminosity\", \"star_feh\"] + [f\"nu_0_{i}\" for i in range(6, 41)]\n",
    "```\n",
    "*! In that order !*\n",
    "\n",
    "Given how well neural networks like `pitchfork` scale to huge batch sizes though (we'll see just how well later on), it's rare that we'd want to predict on just one point at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b23923a-bd1e-4b23-8697-196eb2467b3b",
   "metadata": {},
   "source": [
    "### 1.2. Batched prediction\n",
    "Let's see how we'd predict on a large batch of n=10,000 points each with the same input values that we used before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c98a84b-2d0a-4b5d-a26b-28be5d3d475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_point_dict = {'initial_mass': 1,\n",
    "'initial_Zinit': 0.01,\n",
    "'initial_Yinit': 0.26,\n",
    "'initial_MLT': 2.12,\n",
    "'star_age': 4.52,\n",
    "}\n",
    "\n",
    "single_point_array = np.array(list(single_point_dict.values()))\n",
    "\n",
    "batched_points = np.full((10_000,5),single_point_array)\n",
    "\n",
    "batched_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b099a696-4aba-4828-a98c-6f97d8ca9351",
   "metadata": {},
   "source": [
    "our `batched_points` array already has dimensions (n, 5), so we can just pass this directly to `pitchfork`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabfac1e-872c-49dc-85a5-55d73ba7206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pitchfork.predict(batched_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb1474-126b-465f-85bd-da8867640613",
   "metadata": {},
   "source": [
    "Easy! And that certainly didn't seem 10,000 times slower than the prediction on the single point.\n",
    "\n",
    "Let's see just how much faster *per point* a neural network is at predicting if points are batched like this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce05796-8e1a-4c1e-93c2-bd0e3f02c338",
   "metadata": {},
   "source": [
    "### 1.3. Prediction timing\n",
    "I'm just going to copy the code from above here, and time just the *pitchfork.predict* line for the single and batched cases.\n",
    "\n",
    "Single point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46de30bc-9638-412e-8cf3-37308abb27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_point_dict = {'initial_mass': 1,\n",
    "'initial_Zinit': 0.01,\n",
    "'initial_Yinit': 0.26,\n",
    "'initial_MLT': 2.12,\n",
    "'star_age': 4.52,\n",
    "}\n",
    "\n",
    "single_point_array_fixed = np.array([list(single_point_dict.values())])\n",
    "\n",
    "single_point_start_time = time.perf_counter()\n",
    "pitchfork.predict(single_point_array_fixed)\n",
    "single_point_elapsed_time = time.perf_counter() - single_point_start_time\n",
    "\n",
    "print(f\"Done! Prediction for a single point took: {single_point_elapsed_time*1000:.3f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3703f1d-967c-481d-9ce1-25894675fbe1",
   "metadata": {},
   "source": [
    "And now for the batched case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94bef4d-33dd-42a1-a633-862550b57efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_point_dict = {'initial_mass': 1,\n",
    "'initial_Zinit': 0.01,\n",
    "'initial_Yinit': 0.26,\n",
    "'initial_MLT': 2.12,\n",
    "'star_age': 4.52,\n",
    "}\n",
    "\n",
    "single_point_array = np.array(list(single_point_dict.values()))\n",
    "\n",
    "batched_points = np.full((10_000,5),single_point_array)\n",
    "\n",
    "batched_points_start_time = time.perf_counter()\n",
    "pitchfork.predict(batched_points)\n",
    "batched_points_elapsed_time = time.perf_counter() - batched_points_start_time\n",
    "\n",
    "print(f\"Done! Prediction on {len(batched_points)} points took: {batched_points_elapsed_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59946e4-1e62-48ed-af82-4c5ff3a00807",
   "metadata": {},
   "source": [
    "Let's see whether prediction for the batch of 10,000 points was 10,000 times slower than for the single point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1557e8-adcb-4368-82c4-f5b5d7e479c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Batch of {len(batched_points)} took {batched_points_elapsed_time/single_point_elapsed_time:.0f} times as long as for the single point.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a49f543-b0df-4eea-9ef1-36d7d6c12dc8",
   "metadata": {},
   "source": [
    "Nice! This shows why `pitchfork` is so well suited for a Bayesian inference pipeline that allows for vectorised likelihood evaluation like `UltraNest` :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f626f-824c-42f9-b211-8430a3762070",
   "metadata": {},
   "source": [
    "## 2. Emulating an evolutionary track\n",
    "One neat thing about using an emulator rather than modelling on-the-fly is that we don't have to worry about the forward modelling dependance (tracks being evolved from zero age forwards in consecutive steps of age).\n",
    "\n",
    "And, given we can emulate batches of many points extremely quickly, we can emulate an arbitrarily dense evolutionary track for a given set of fundamental properties nearly instantly!\n",
    "\n",
    "Let's make a function to do that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bab9a7-7fa8-4d94-8e8d-b21d95eb47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emulate_track(pitchfork, initial_mass, initial_Zinit, initial_Yinit, initial_MLT, n_points=100_000, min_age = 0.03, max_age = 10):\n",
    "    fundamental_params = [initial_mass, initial_Zinit, initial_Yinit, initial_MLT]\n",
    "    \n",
    "    zero_age_point = np.array(fundamental_params+[0])\n",
    "\n",
    "    track = np.full((n_points,5), zero_age_point)\n",
    "\n",
    "    ages = np.linspace(min_age, max_age, n_points)\n",
    "\n",
    "    track[:,4] = ages\n",
    "\n",
    "    emulated_track = pitchfork.predict(track)\n",
    "\n",
    "    emulated_teff = emulated_track[:,0]\n",
    "    \n",
    "    emulated_L = emulated_track[:,1]\n",
    "    \n",
    "    plt.scatter(emulated_teff, np.log10(emulated_L), c=ages, s=8, cmap = 'winter')\n",
    "    cbar = plt.colorbar()\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.xlabel(r'$T_{\\text{eff}},\\,\\text{K}$')\n",
    "    \n",
    "    plt.ylabel(r'$\\text{log}L, \\text{dex}$')\n",
    "    cbar.set_label('Age, Gyr')\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c57043-1fba-4e6f-b46e-4f076974041b",
   "metadata": {},
   "source": [
    "and now we can try to emulate a sparse track with just 100 evenly spaced points in age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67601971-c19b-4339-9f65-da0e7061a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "emulate_track(pitchfork, 1, 0.01, 0.26, 2.12, n_points=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61497c1d-5061-404c-8551-d077f6477c1f",
   "metadata": {},
   "source": [
    "or a densely sampled track with 100,000 points in age:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0b146e-4d42-4d3a-b335-a6de3b905fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emulate_track(pitchfork, 1, 0.01, 0.26, 2.12, n_points=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa6999-3718-44f9-923d-9eee2a11619c",
   "metadata": {},
   "source": [
    "And we can easily change the input values and try again - what about a star right at the upper limit of `pitchfork`'s trained range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e857bc6-1056-4b02-91d5-3ce535600f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "emulate_track(pitchfork, 1.2, 0.01, 0.26, 2.12, max_age = 6, n_points=100_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96dd688-1865-457a-837b-791ff75a945e",
   "metadata": {},
   "source": [
    "## 3. Posterior Predictive Check\n",
    "Another cool thing we can do with `pitchfork` is a posterior predictive check!\n",
    "\n",
    "When we've run our `pitchfork` sampler, we end up with posterior samples in the input domain for `pitchfork` (i.e. `['initial_mass', 'initial_Zinit', 'initial_Yinit', 'initial_MLT', 'star_age']`).\n",
    "\n",
    "Even though our posteriors can contain thousands of samples, we've demonstrated above that `pitchfork` can make a prediction on all of these in milliseconds!\n",
    "\n",
    "The result are posterior predicted observables for each set of posterior samples, which we can then use to make sure our posterior predicted observables do actually match up with the data used to sample the posterior in the first place.\n",
    "\n",
    "Let's take a quick look at how we could get some posterior predictions from saved solar data and sampling results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e300fc-d8d1-40ec-8300-96160c06bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stars/Sun/Sun.json', 'r') as fp:\n",
    "    Sun_data = json.load(fp)\n",
    "\n",
    "with open('stars/Sun/Sun_results.pkl', 'rb') as fp:\n",
    "    Sun_samples = pickle.load(fp)['samples']\n",
    "\n",
    "Sun_input_samples = Sun_samples[:,:5]\n",
    "\n",
    "posterior_preds = pitchfork.predict(Sun_input_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be07d123-cfd0-4124-b019-942fa5f6e3bd",
   "metadata": {},
   "source": [
    "Done! Now to perform our posterior predictive check we can compare to the data used by plotting like so (I crop to just n=22 to keep corner dimensions down):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86919022-d049-4bcf-8ed4-1a85b0fde4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"calc_effective_T\", \"luminosity\", \"star_feh\", 'nu_0_22']\n",
    "\n",
    "truths = [Sun_data[label][0] for label in labels]\n",
    "\n",
    "cropped_posterior_preds = np.column_stack((posterior_preds[:,:3], posterior_preds[:,19]))\n",
    "fig = corner.corner(cropped_posterior_preds, labels=labels, color='#D33682', smooth=True, truths=truths, show_titles = True, truth_color = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868d5ca-e0c7-4f13-b71a-e6d32c3143a5",
   "metadata": {},
   "source": [
    "Pretty good! You may notice that our posterior predictive distribution for n=22 is slightly off - but this is expected!\n",
    "\n",
    "Check the paper for the gritty details, but basically there is a \"surface correction\" that must be applied to simulated (and therefore emulated) mode frequencies for them to match observed modes which comes from our inability to properly model the near-surface layers of the star.\n",
    "\n",
    "This is what the `a` and `b` parameters that we sample for - they parameterise the surface correction. Again, I won't go into too much detail here, but here's a repeated posterior predictive check with the sampled `a` and `b` used to correct the posterior predicted mode frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c740095-2a5f-4ad3-a50d-4106ac028f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def surface_correction(freq_arr, a_arr, b_arr, nu_max):\n",
    "    return freq_arr + a_arr*((freq_arr/nu_max)**b_arr)\n",
    "\n",
    "a_arr = np.expand_dims(Sun_samples[:,5],1)\n",
    "b_arr = np.expand_dims(Sun_samples[:,6],1)\n",
    "\n",
    "nu_max = Sun_data['nu_max'][0]\n",
    "\n",
    "freq_array = posterior_preds[:,3:]\n",
    "\n",
    "corrected_freq_array = surface_correction(freq_array, a_arr, b_arr, nu_max)\n",
    "\n",
    "corrected_posterior_preds = np.concatenate((posterior_preds[:,:3], corrected_freq_array), axis=1)\n",
    "\n",
    "cropped_corrected_posterior_preds = np.column_stack((corrected_posterior_preds[:,:3], corrected_posterior_preds[:,19]))\n",
    "\n",
    "fig = corner.corner(cropped_corrected_posterior_preds, labels=labels, color='#D33682', smooth=True, truths=truths, show_titles = True, truth_color = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db70881c-6b66-4071-a5f9-c0fa6359b56d",
   "metadata": {},
   "source": [
    "Nice :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
